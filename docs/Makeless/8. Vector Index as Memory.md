#LLM #Memory 

> Reference: https://github.com/qdrant/qdrant

### 1. Introduction

Vector index (or database) enables fast and approximate similarity search on embedding.

Vector databases are widely used for semantic search, image retrieval, recommendation and extreme classification tasks.

Vector index providers normally don't make full ACID guarantees on the underlying data collection. qdrant supports fast ANN query with filtering. 

In the time of LLM, the flexibility and scalability of vector databases make them the perfect *memory* for accessing LLM via API.

This *memory* with fast IO performance enables:
- prompt-response cache
- multi-document search
- retrieval and summary
- choice from similar entities
- comparison and playback


### 2. Usage
Set up the service with docker:
```yaml
version: "3.8"
services:
  qdrant:
    image: qdrant/qdrant
    container_name: qdrant_server
    ports:
      - "6333:6333"
    volumes:
	    - ./qdrant_index:/qdrant/storage
	    - ./config.yaml:/qdrant/config/production.yaml
    restart: always
```


### 3. Verdict

| View          | Score | Comment                                 |
| ------------- | ----- | --------------------------------------- |
| Functionality | 4     | ANN with additional filtering support.  |
| Usability     | 5     | Simple to setup and wrap around.        |
| Stability     | 4     | Clear roadmap to support more features. |
| Extensibility | 2     | Written in Rust.                        |
| Safety        | 5     | Used in cloud solution.                 |


#LLM #Downstream

> Reference: https://github.com/hwchase17/langchain.git

### 1. Introduction

LangChain provides useful modular functionalities that empowers many downstream tasks with LLM.

LangChain is first built to avoid the context limit of LLM inference request in search tasks.

Due to the generative feature of LLM, LangChain conceptualise the sequential interactions with model inference endpoint as a **chain** of actions.

In addition to the basic components including LLM, prompts and different memory mechanism, LangChain provides **agents** as pre-defined arms of LLM to external systems.

For LLMs that provide multi-round chat functions, LangChain follows the convention to support three roles in the conversation: *system*, *human* and the *AI*.


### 2. Usage
```python
!pip install langchain

from langchain import ConversationChain, OpenAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatMessagePromptTemplate

cache = ConversationBufferMemory()
openai=OpenAI()

chain = ConversationChain(
	memory=cache,
	llm=openai
)

chain.run("what is love?")
```


### 3. Verdict

| View          | Score | Comment                                      |
| ------------- | ----- | -------------------------------------------- |
| Functionality | 5     | Good coverage on downstream tasks.           |
| Usability     | 4     | Plug-in development experience.              |
| Stability     | 2     | APIs and some modules are actively evolving. |
| Extensibility | 4     | Well-designed sub-modules.                   |
| Safety        | 5     | Fully open-sourced and well-tested.          |

